\documentclass[12pt]{article}
\renewcommand{\baselinestretch}{1.1}

%%% Add packages here
\usepackage{graphicx} 
\usepackage{parskip}
\usepackage[table]{xcolor}
\usepackage{amsmath}
\usepackage{float}
\usepackage{geometry}
\geometry{legalpaper, portrait, margin=3cm}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{array}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{color}
\usepackage{verbatim}
\usepackage{fancyhdr}
\usepackage{fancybox}
\usepackage{listings}
\usepackage{threeparttable} % for table notes
\usepackage{hologo} % for TeX engine logos
\usepackage{booktabs} % for nice tables
\usepackage{longtable} % for longer tables
\usepackage[table,xcdraw]{xcolor}  % for color in tables
\usepackage{appendix} % for appendix
\usepackage{biblatex}
\addbibresource{references.bib}

\usepackage{amsmath} % for pseudo code
\usepackage{algorithm}
\usepackage{algpseudocode}

%%% Margins
\addtolength{\oddsidemargin}{-.50in}
\addtolength{\evensidemargin}{-.50in}
\addtolength{\textwidth}{1.0in}
\addtolength{\topmargin}{-.40in}
\addtolength{\textheight}{0.80in}

%%% Header
\pagestyle{fancy}
\chead{} % Clear undefined \groupname
\rhead{}
\lhead{}
\cfoot{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\clearpage\thispagestyle{empty}

\begin{titlepage}

\begin{center}
\vspace*{1in}
	% title
	\centering\textbf{\huge{Bayesian data analysis project\\[1cm]
 }} \\[2cm]
	% details
	\Large{
	Master of Statistics and Data Science \\
	Hasselt University\\	
        2024-2025 \\
	}


\vspace*{3cm}
\textbf{\large{Group :}}\\
Luong Vuong (2365900) \\
Edward Owinoh (2365191) \\
Joseph Kaunda (2364031) \\



\vspace*{2cm}


\vspace*{1.5cm}
\textbf{\large{Lecturers:}}\\
Prof. Dr. Christel Faes \\
Dr. Oswaldo Gressani \\


\vspace*{2\baselineskip}
\today

\begin{figure}[b]
   \centering
   \includegraphics[width=8cm]{pictures/UHasselt_logog.png}
   \label{fig:Uhasselt}
\end{figure}

\end{center}

\end{titlepage}

\section{Part 1}

\section{Part 2 - The Gibbs algorithm}
Let \(\boldsymbol{\theta} = (\theta_1, \theta_2)^\top \in \mathbb{R}^2\) and consider the following (unscaled) posterior density:

\[
p(\boldsymbol{\theta} \mid \mathcal{D}) \propto \exp\left(-8\theta_1^2\theta_2^2 - 0.5\theta_1^2 - 0.5\theta_2^2 + \cos(2\pi + 0.3)\theta_1\theta_2 + 0.3\theta_1 + 0.2\theta_2 \right).
\]

\subsection{Derive the conditional posteriors \(p(\theta_1 \mid \theta_2, \mathcal{D})\) and \(p(\theta_2 \mid \theta_1, \mathcal{D})\). To what well-known parametric family do these conditional posterior distributions belong? Explain mathematically how you computed the conditional posterior mean \(E(\theta_1 \mid \theta_2, \mathcal{D})\) and the conditional posterior variance \(V(\theta_1 \mid \theta_2, \mathcal{D})\).} 
\subsubsection{Solution \(p(\theta_1 \mid \theta_2, \mathcal{D})\) }{\label{theta1}}
We first focus on deriving the conditional posterior of  \(p(\theta_1 \mid \theta_2, D)\) using the steps below:

\begin{enumerate}
\item Re-arranging terms from the joint posterior density with respect to $\theta_1$ terms we get:

\begin{equation}\label{theta1eq1}
p(\theta \mid \mathcal{D}) \propto \exp \left( -8\theta_1^2\theta_2^2 - 0.5\theta_1^2 + \theta_1(\cos(2\pi + 0.3)\theta_2 + 0.3) \right) \times \exp \left( -0.5\theta_2^2 + 0.2\theta_2 \right)  
\end{equation} 
\item Since  $\exp \left( -0.5\theta_2^2 + 0.2\theta_2 \right)$  is not a function dependent on $\theta_1$ in eqn  \ref{theta1eq1}, $p(\theta_1 \mid \theta_2 ,\mathcal{D}) $ can then be written as:
 \begin{equation}\label{theta1eq2}
p(\theta_1 \mid \theta_2 ,\mathcal{D}) \propto \exp \left( -8\theta_1^2\theta_2^2 - 0.5\theta_1^2 + \theta_1(\cos(2\pi + 0.3)\theta_2 + 0.3) \right)
\end{equation} 

This is a quadratic function of $\theta_1$ and can be rewritten as   \begin{equation}\label{theta1eq3}
p(\theta_1 \mid \theta_2 ,\mathcal{D}) \propto \exp\left[- \frac{1}{2}\left( 16\theta_1^2\theta_2^2 + \theta_1^2 - 2*(\cos(2\pi + 0.3)\theta_2 \theta_1) - 2*(0.3\theta_1 \right)\right]
\end{equation} 

Which is the general kernel of the Gaussian distribution. 
\end{enumerate}

$[ f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right) ]$


\subsubsection{Solution \(p(\theta_2 \mid \theta_1, \mathcal{D})\) }
To derive the conditional posterior \(p(\theta_2 \mid \theta_1, \mathcal{D})\), we repeat the steps in section \ref{theta1} but with a focus on $\theta_2$:

\begin{enumerate}
    \item From the joint posterior density we re-arrange the equation w.r.t $\theta_2$ terms as:
\begin{equation}\label{theta2eq1}
p(\theta \mid \mathcal{D}) \propto \exp \left( -8\theta_1^2\theta_2^2 - 0.5\theta_2^2 + \theta_2(\cos(2\pi + 0.3)\theta_1 + 0.2) \right) \times \exp \left( -0.5\theta_1^2 + 0.3\theta_1 \right) 
\end{equation} 
\item Since  $\exp  \left( -0.5\theta_1^2 + 0.3\theta_1 \right)$  is not a function dependent on $\theta_2$ in eqn  \ref{theta2eq1}, $p(\theta_2 \mid \theta_1 ,\mathcal{D}) $ can then be written as:
 \begin{equation}\label{theta2eq2}
p(\theta_2 \mid \theta_1 ,\mathcal{D}) \propto \exp \left( -8\theta_1^2\theta_2^2 - 0.5\theta_2^2 + \theta_2(\cos(2\pi + 0.3)\theta_1 + 0.2) \right)
\end{equation} 
a quadratic function of $\theta_2$ and could then be rewritten as:
 \begin{equation}\label{theta2eq3}
p(\theta_2 \mid \theta_1 ,\mathcal{D}) \propto \exp\left[- \frac{1}{2}\left( 16\theta_1^2\theta_2^2 + \theta_2^2 - 2*(\cos(2\pi + 0.3)\theta_2 \theta_1) - 2*(0.2\theta_2 \right)\right]
\end{equation} which is also a general form of the Gaussian distribution.
   
\end{enumerate}
\subsubsection{Conditional posterior mean \(E(\theta_2 \mid \theta_1, \mathcal{D})\) and the conditional posterior variance \(V(\theta_1 \mid \theta_2, \mathcal{D})\).}

\begin{enumerate}
\item We arrange the exponent in equation \ref{theta1eq3} according to power terms of $\theta_1$ and define $X$ and $Y$ as shown in equation \ref{condtheta1}:
\begin{equation}\label{condtheta1}
-\frac{1}{2} \left( \underbrace{(16\theta_2^2 + 1)}_{X}\theta_1^2 - 2\underbrace{\left(\cos(2\pi + 0.3)\theta_2 + 0.3\right)}_{Y}\theta_1 \right)
\end{equation}

\item Using the completing square method we can rewrite the terms in the brackets in equation \ref{condtheta1} as 

\begin{equation}\label{condtheta2}
     X\theta_1^2 - 2Y\theta_1 = X\left(\theta_1 - \frac{Y}{X}\right)^2 - \frac{Y^2}{X}
\end{equation}
\item Substituting \( X = 16\theta_2^2 + 1 \) and \( Y = \cos(2\pi + 0.3)\theta_2 + 0.3 \) in eqn \ref{condtheta2} the full conditional posterior density  can be written as:

\begin{equation} \label{condtheta3}
p(\theta_1 \mid \theta_2, \mathcal{D}) \propto \exp\left[-\frac{1}{2}(16\theta_2^2 + 1)\left(\theta_1 - \frac{\cos(2\pi + 0.3)\theta_2 + 0.3}{16\theta_2^2 + 1}\right)^2\right]
\end{equation}

which is a kernel from the Gaussian Distribution with Mean   $\mathbb{E}(\theta_1 \mid \theta_2, \mathcal{D}) = \frac{\cos(2\pi + 0.3)\theta_2 + 0.3}{16\theta_2^2 + 1}$ and variance  $  \mathrm{Var}(\theta_1 \mid \theta_2, \mathcal{D}) = \frac{1}{16\theta_2^2 + 1}$
\end{enumerate}
\subsection{ Based on the full conditionals obtained in the previous step, write a pseudo-code to obtain a random sample of total size 50,000 (including a burn-in of size 20,000) from $p(\theta | \mathcal{D})$ using the Gibbs sampler.} 
\subsubsection{Solution}
We firstly have to mathematically compute the \(E(\theta_2 \mid \theta_1, \mathcal{D})\) and the conditional posterior variance \(V(\theta_2 \mid \theta_1, \mathcal{D})\). Using the general example, if you have a conditional prior in the form of $
p(\theta_2 \mid \theta_1, \mathcal{D}) \propto \exp\left\{ -\frac{1}{2} \left( A\theta_1^2\theta_2^2 + \theta_2^2 - 2B\theta_1\theta_2 - 2C_2\theta_2 \right) \right\}.
$ this implies that$\theta_2 \mid \theta_1, \mathcal{D} \sim \mathcal{N} \left( \frac{B\theta_1 + C_2}{1 + A\theta_1^2}, \frac{1}{1 + A\theta_1^2} \right)$ \cite{gelman1995bayesian} \cite{author2024notes}. Substituting this in equation \ref{theta2eq3}, then we can conclude that $\theta_2 \mid \theta_1, \mathcal{D} \sim \mathcal{N} \left( \frac{\cos(2\pi + 0.3)\theta_1 + 0.2}{16\theta_1^2 + 1}, \frac{1}{16\theta_1^2 + 1}\right)$ 

The full two stage Gibbs sample pseudo-code is as shown below:
\begin{algorithm}
Gibbs Sampler to draw from \( p(\boldsymbol{\theta} \mid \mathcal{D}) \)
\begin{algorithmic}[1]

\State Fix initial values \( \theta_1 = \theta_1^{(0)} \), \( \theta_2 = \theta_2^{(0)} \).
\For{\( i = 1 \), \ldots , \( 50000 \)}
    \State Sample \( \theta_1^{(i)} \sim \mathcal{N}\left(\frac{\cos(2\pi + 0.3)\theta_2^{(i-1)} + 0.3}{16(\theta_2^{(i-1)})^2 + 1)}, \frac{1}{2(8(\theta_2^{(i-1)})^2 + 1)}\right) \).
    \State Sample \( \theta_2^{(i)} \sim \mathcal{N}\left(\frac{\cos(2\pi + 0.3)\theta_1^{(i)} + 0.2}{16(\theta_1^{(i)})^2 + 1)}, \frac{1}{16(\theta_1^{(i)})^2 + 1)}\right) \).
\EndFor
\State Discard the first \( b = 20,000 \) samples as burn-in
\State Return \( \{\theta_1^{(i)}, \theta_2^{(i)}\}_{i=B+1}^{N} \) which under mild regularity and if convergence occurs can be said to be drawn from \( p(\boldsymbol{\theta} \mid \mathcal{D}) \).
\end{algorithmic}
\end{algorithm}
\subsection{Using R and without relying on external software/packages (except the \texttt{coda} package), write the Gibbs sampler based on your pseudo-code in step }

\subsection{Please specify \texttt{set.seed(2025)} at the beginning of your code for reproducibility of your results. What is your acceptance rate?}

\subsection{Use the Geweke convergence diagnostic test on the generated chains and interpret your results.}

\subsection{Compute a point estimate for $\theta_1$ and a 95\% quantile-based credible interval}

\subsection{Using your generated chains, estimate the probability $P(\theta_1 > 0.5)$.}



\section{Part 2 - The Metropolis algorithm}
\subsection{Write the log posterior distribution \( \log p(\theta|D) \) and obtain analytically the gradient
\[\nabla_\theta \log p(\theta|D) = \left( \frac{\partial \log p(\theta|D)}{\partial \theta_1}, \frac{\partial \log p(\theta|D)}{\partial \theta_2} \right)^\top \]
and Hessian matrix:
\[\nabla^2_\theta \log p(\theta|D) = 
\begin{pmatrix}
\frac{\partial^2 \log p(\theta|D)}{\partial \theta_1^2} & \frac{\partial^2 \log p(\theta|D)}{\partial \theta_1 \partial \theta_2} \\
\frac{\partial^2 \log p(\theta|D)}{\partial \theta_2 \partial \theta_1} & \frac{\partial^2 \log p(\theta|D)}{\partial \theta_2^2} 
\end{pmatrix}.\]}
\textbf{Solution}\\
The log posterior distribution could be written as:
\[
\log(p(\theta|D)) \propto -8\theta_1^2\theta_2^2 - 0.5\theta_1^2 - 0.5\theta_2^2 + \cos(2\pi + 0.3)\theta_1\theta_2 + 0.3\theta_1 + 0.2\theta_2
\]
By taking the first and second-order partial derivatives with regard to \(\theta_1\) and \(\theta_2\), of \(\log p(\theta|D)\),
we obtain the gradient as follows:
\[
\nabla_\theta \log p(\theta|D) = \left( -16\theta_1\theta_2^2 - \theta_1 + \cos(2\pi + 0.3)\theta_2 + 0.3, -16\theta_1^2\theta_2 - \theta_2 + \cos(2\pi + 0.3)\theta_1 + 0.2 \right)^\top
\]
And the Hessian matrix is as follows:
\[
\nabla^2_\theta \log p(\theta|D) = 
\begin{pmatrix}
-16\theta_2^2 - 1 & -32\theta_1\theta_2 + \cos(2\pi + 0.3) \\
-32\theta_1\theta_2 + \cos(2\pi + 0.3) & -16\theta_1^2 - 1
\end{pmatrix}
\]

\subsection{Using the gradient and Hessian matrix obtained in the previous step, implement a Newton-Raphson algorithm in R to find the posterior mode of \( p(\theta|D) \) and denote by
\[
\theta^*_{NR} = (\theta^*_1, \theta^*_2)^\top
\]
the mode after convergence of the algorithm. (Note: round \( \theta^*_{NR} \) to five digits after the decimal point).}
\textbf{Solution}
The Pseudo-code of Newton-Raphson algorithm to find the mode of \( p(\theta|D) \) is as follows:

\begin{algorithm}
\begin{algorithmic}[1]
\State Set tolerance \( \epsilon \), arbitrary initial distance \( d \), initialize \( \theta^{(0)} \) and set \( m = 0 \).
\While{$d > \epsilon$}
    \State \( \theta^{(m+1)} = \theta^{(m)} - \left( \nabla^2 \log p(\theta^{(m)}|D) \right)^{-1} \nabla \log p(\theta^{(m)}|D) \)
    \State Compute distance \( d = \|\theta^{(m+1)} - \theta^{(m)}\| \).
\EndWhile
\State At convergence, return \( \theta^M \).
\end{algorithmic}
\end{algorithm}

\subsection{Compute a Laplace approximation to \( p(\theta|D) \) around \( \theta^*_{NR} \). Report the covariance matrix \( \Sigma^* \) of the Laplace approximation. (Note: round your results to five digits after the decimal point).}
\textbf{Solution}

\subsection{In R, write a random-walk Metropolis algorithm to explore the joint posterior \( p(\theta|D) \) using a Gaussian proposal with covariance matrix \( \tilde{\Sigma} = c\Sigma^* \). Use a chain of length \( M = 50,000 \) and tune \( c \) to (approximately) reach the optimal acceptance rate of 23\%. Please specify \textit{set.seed(1993)} for the sake of reproducibility of your results.}
\textbf{Solution}

\subsection{Using your generated chains, estimate the probability \( P\left(\frac{\theta_1}{\theta_2} > 0.45\right) \).}
\textbf{Solution}

\printbibliography

\end{document}
