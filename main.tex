\documentclass[12pt]{article}
\renewcommand{\baselinestretch}{1.1}

%%% Add packages here
\usepackage{graphicx} 
\usepackage{parskip}
\usepackage[table]{xcolor}
\usepackage{amsmath}
\usepackage{float}
\usepackage{geometry}
\geometry{legalpaper, portrait, margin=3cm}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{array}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{color}
\usepackage{verbatim}
\usepackage{fancyhdr}
\usepackage{fancybox}
\usepackage{listings}
\usepackage{threeparttable} % for table notes
\usepackage{hologo} % for TeX engine logos
\usepackage{booktabs} % for nice tables
\usepackage{longtable} % for longer tables
\usepackage[table,xcdraw]{xcolor}  % for color in tables
\usepackage{appendix} % for appendix
\usepackage{biblatex}
\addbibresource{references.bib}

\usepackage{amsmath} % for pseudo code
\usepackage{algorithm}
\usepackage{algpseudocode}

%%% Margins
\addtolength{\oddsidemargin}{-.50in}
\addtolength{\evensidemargin}{-.50in}
\addtolength{\textwidth}{1.0in}
\addtolength{\topmargin}{-.40in}
\addtolength{\textheight}{0.80in}

%%% Header
\pagestyle{fancy}
\chead{} % Clear undefined \groupname
\rhead{}
\lhead{}
\cfoot{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\clearpage\thispagestyle{empty}

\begin{titlepage}

\begin{center}
\vspace*{1in}
	% title
	\centering\textbf{\huge{Bayesian data analysis project\\[1cm]
 }} \\[2cm]
	% details
	\Large{
	Master of Statistics and Data Science \\
	Hasselt University\\	
        2024-2025 \\
	}


\vspace*{3cm}
\textbf{\large{Group :}}\\
Luong Vuong (2365900) \\
Edward Owinoh (2365191) \\
Joseph Kaunda (2364031) \\



\vspace*{2cm}


\vspace*{1.5cm}
\textbf{\large{Lecturers:}}\\
Prof. Dr. Christel Faes \\
Dr. Oswaldo Gressani \\


\vspace*{2\baselineskip}
\today

\begin{figure}[b]
   \centering
   \includegraphics[width=8cm]{pictures/UHasselt_logog.png}
   \label{fig:Uhasselt}
\end{figure}

\end{center}

\end{titlepage}

\section{Part 1}

\section{Part 2 - The Gibbs algorithm}
Let \(\boldsymbol{\theta} = (\theta_1, \theta_2)^\top \in \mathbb{R}^2\) and consider the following (unscaled) posterior density:

\[
p(\boldsymbol{\theta} \mid D) \propto \exp\left(-8\theta_1^2\theta_2^2 - 0.5\theta_1^2 - 0.5\theta_2^2 + \cos(2\pi + 0.3)\theta_1\theta_2 + 0.3\theta_1 + 0.2\theta_2 \right).
\]

\subsection{Derive the conditional posteriors \(p(\theta_1 \mid \theta_2, D)\) and \(p(\theta_2 \mid \theta_1, D)\). To what well-known parametric family do these conditional posterior distributions belong? Explain mathematically how you computed the conditional posterior mean \(E(\theta_1 \mid \theta_2, D)\) and the conditional posterior variance \(V(\theta_1 \mid \theta_2, D)\).} 
\subsubsection{Solution \(p(\theta_1 \mid \theta_2, D)\) }{\label{theta1}}
We first focus on deriving the conditional posterior of  \(p(\theta_1 \mid \theta_2, D)\) using the steps below:

\begin{enumerate}
\item Re-arranging terms from the joint posterior density with respect to $\theta_1$ terms we get:

\begin{equation}\label{theta1eq1}
p(\theta \mid D) \propto \exp \left( -8\theta_1^2\theta_2^2 - 0.5\theta_1^2 + \theta_1(\cos(2\pi + 0.3)\theta_2 + 0.3) \right) \times \exp \left( -0.5\theta_2^2 + 0.2\theta_2 \right)  
\end{equation} 
\item Since  $\exp \left( -0.5\theta_2^2 + 0.2\theta_2 \right)$  is not a function dependent on $\theta_1$ in eqn  \ref{theta1eq1}, then $p(\theta_1 \mid \theta_2 ,D) $ can then be written as:
 \begin{equation}\label{theta1eq2}
p(\theta_1 \mid \theta_2 ,D) \propto \exp \left( -8\theta_1^2\theta_2^2 - 0.5\theta_1^2 + \theta_1(\cos(2\pi + 0.3)\theta_2 + 0.3) \right)
\end{equation}    
\item Grouping the power terms together and letting $x$ be $\left(8\theta_2 +0.5\right)$  and $y$ be $\left(\cos(2\pi + 0.3)+ 0.3\right)$ eqn \ref{theta1eq2} is written as:
 \begin{equation}\label{theta1eq3}
p(\theta_1 \mid \theta_2 ,D) \propto \exp \left( -x\theta_1^2\ + \theta_1(\cos(2\pi + 0.3)\theta_2 + 0.3) \right)
\end{equation}   

    \[\exp\left(-\theta_1^2\left(8 +0.5\right)+ \theta_1 \left(\cos(2\pi + 0.3)+ 0.3\right)\right).\]
    \item let $x$ be $\left(8 +0.5\right)$ and $y$ be $\left(\cos(2\pi + 0.3)+ 0.3\right)$; equation (3) can be simplified as $\exp\left(-\theta_1^2x+ \theta_1y\right)$ which is a quadratic function of $\theta_1$
    \item factoring out $-x$ we rewrite the bracket in (4) as $-x\theta_1^2 + y\theta_1 = -x\left(\theta_1^2 - \frac{y}{x}\theta_1\right)$
    \item Using the completing square method and substituting $-x$ back we get
    \[
-x\theta_1^2 + y\theta_1 = -x\left[\left(\theta_1 - \frac{y}{2x}\right)^2 - \left(\frac{y}{2x}\right)^2\right].
\]
 \item Expanding and simplifying (6) above
 \[
-x\theta_1^2 + y\theta_1 = -x\left(\theta_1 - \frac{y}{2x}\right)^2 + \frac{y^2}{4x}
\]
\item Substituting everything back to the exponential we get:
\[
p(\theta_1 \mid \theta_2, D) \propto \exp\left(-x\left(\theta_1 - \frac{y}{2x}\right)^2 + \frac{y^2}{4x}\right).
\]
\item Since \(\frac{y^2}{4x}\) is independent of \(\theta_1\), this could be treated as a constant and the equation rewritten as:
\[
p(\theta_1 \mid \theta_2, D) \propto \exp\left(-x\left(\theta_1 - \frac{y}{2x}\right)^2\right), \text {where x} =\left(8 +0.5\right) \text { and y} = \left(\cos(2\pi + 0.3)+ 0.3\right)\] which correspond to the Gaussian Kernel. 

\end{enumerate}

\subsubsection{Solution \(p(\theta_2 \mid \theta_1, D)\) }
To derive the conditional posterior \(p(\theta_2 \mid \theta_1, D)\), we repeat the steps in section \ref{theta1} but with a focus on $\theta_2$ with $\theta_1$ treated as a constant.

\begin{enumerate}
    \item From
$p(\boldsymbol{\theta} \mid D) \propto \exp\left(-8\theta_1^2\theta_2^2 - 0.5\theta_1^2 - 0.5\theta_2^2 + \cos(2\pi + 0.3)\theta_1\theta_2 + 0.3\theta_1 + 0.2\theta_2 \right)$ we fix $\theta_1$ ,isolate terms with $\theta_2$ and the r.h.s could be written as:
\[\exp\left(-8\theta_2^2 - 0.5\theta_2^2 + \cos(2\pi + 0.3)\theta_2 + 0.2\theta_2 \right)\]
    \item let $x$ be $\left(8 +0.5\right)$ and $y$ be $\left(\cos(2\pi + 0.3)+ 0.2\right)$; the above equation can be simplified as $\exp\left(-\theta_1^2x+ \theta_1y\right)$ which is also a quadratic function of $\theta_2$
    \item simplifying using steps 5 to 8 as in section \ref{theta1}, we get
    \[ p(\theta_2 \mid \theta_1, D) \propto \exp\left(-x\left(\theta_2 - \frac{y}{2x}\right)^2\right), \text {where x} =\left(8 +0.5\right) \text { and y} = \left(\cos(2\pi + 0.3)+ 0.2\right)\] 
    which correspond to the Gaussian Kernel
\end{enumerate}
\subsubsection{Conditional posterior mean \(E(\theta_1 \mid \theta_2, D)\) and the conditional posterior variance \(V(\theta_1 \mid \theta_2, D)\).}
\begin{enumerate}
    \item Starting with a the unscaled joint posterior density \[
p(\boldsymbol{\theta} \mid D) \propto \exp\left(-8\theta_1^2\theta_2^2 - 0.5\theta_1^2 - 0.5\theta_2^2 + \cos(2\pi + 0.3)\theta_1\theta_2 + 0.3\theta_1 + 0.2\theta_2 \right).\] we start with only terms involving $\theta_1$ and rewrite the equation as: \[p(\theta_1 \mid \theta_2, D) \propto \exp\left(-8\theta_1^2\theta_2^2 - 0.5\theta_1^2 + \cos(2\pi + 0.3)\theta_1\theta_2+ 0.3\theta_1 \right) \]
    \item  Grouping the power terms of $\theta_1$ together and rewrite the r.h.s as:
    \[\exp\left((-8\theta_2^2 - 0.5)\theta_1^2 + (\cos(2\pi + 0.3)\theta_2+ 0.3)\theta_1 \right)\]
    \item let $x$ be $(8\theta_2^2 - 0.5)$ and $y$ be $(\cos(2\pi + 0.3)\theta_2+ 0.3)$; equation in 2  can be simplified as $\exp\left(-\theta_1^2x+ \theta_1y\right)$ which is a quadratic function of $\theta_1$
    \item factoring out $-x$ we rewrite the bracket in (4) as $-x\theta_1^2 + y\theta_1 = -x\left(\theta_1^2 - \frac{y}{x}\theta_1\right)$
    \item Using the completing square method and substituting $-x$ back we get
    \[
-x\theta_1^2 + y\theta_1 = -x\left[\left(\theta_1 - \frac{y}{2x}\right)^2 - \left(\frac{y}{2x}\right)^2\right].
\]
 \item Expanding and simplifying (5) above
 \[
-x\theta_1^2 + y\theta_1 = -x\left(\theta_1 - \frac{y}{2x}\right)^2 + \frac{y^2}{4x}
\]
\item Substituting everything back to the exponential we get:
\[
p(\theta_1 \mid \theta_2, D) \propto \exp\left(-x\left(\theta_1 - \frac{y}{2x}\right)^2 + \frac{y^2}{4x}\right).
\]
\item Since \(\frac{y^2}{4x}\) is independent of \(\theta_1\), this could be treated as a constant and the equation rewritten as:
\[
p(\theta_1 \mid \theta_2, D) \propto \exp\left(-x\left(\theta_1 - \frac{y}{2x}\right)^2\right), \text {where x and y are as defined in step 3}.\] 
\item Rewriting the expression in the general gaussian form \[p(\theta_1 \mid \theta_2, D) \propto \exp\left(-\frac{\left(\theta_1 - \mu_2\right)^2}{2\sigma_2^2}\right)\] and substituting the values of $x$ and $y$ in  $\mu_{\theta_1}$ and    $\sigma_{\theta_1}^2$ we get
\[
p \left(\theta_1 \mid \theta_2, D \right)\sim \mathcal{N}\left(-\frac{\cos(2\pi + 0.3)\theta_2 + 0.2}{2(8\theta_2^2 + 0.5)}, \frac{1}{2(8\theta_2^2 + 0.5)}\right).
\]
\end{enumerate} 

Thus the  Conditional posterior mean \(E(\theta_1 \mid \theta_2, D)\)  is $-\frac{\cos(2\pi + 0.3)\theta_2 + 0.3}{2(8\theta_2^2 + 0.5)}$ and  conditional posterior variance \(V(\theta_1 \mid \theta_2, D)\) is $\frac{1}{2(8\theta_2^2 + 0.5)}$

\subsection{ Based on the full conditionals obtained in the previous step, write a pseudo-code to obtain a random sample of total size 50,000 (including a burn-in of size 20,000) from $p(\theta | D)$ using the Gibbs sampler.} 
\begin{algorithm}
Gibbs Sampler to draw from \( p(\boldsymbol{\theta} \mid D) \)
\begin{algorithmic}[1]

\State Fix initial values \( \theta_1 = \theta_1^{(0)} \), \( \theta_2 = \theta_2^{(0)} \).

\For{\( i = 1 \), \ldots , \( 50000 \)}
    \State Sample \( \theta_1^{(i)} \sim \mathcal{N}\left(-\frac{\cos(2\pi + 0.3)\theta_2^{(i-1)} + 0.3}{2(8(\theta_2^{(i-1)})^2 + 0.5)}, \frac{1}{2(8(\theta_2^{(i-1)})^2 + 0.5)}\right) \).
    \State Sample \( \theta_2^{(i)} \sim \mathcal{N}\left(-\frac{\cos(2\pi + 0.3)\theta_1^{(i)} + 0.2}{2(8(\theta_1^{(i)})^2 + 0.5)}, \frac{1}{2(8(\theta_1^{(i)})^2 + 0.5)}\right) \).
\EndFor
\State Discard the first \( b = 20,000 \) samples as burn-in
\State Return \( \{\theta_1^{(i)}, \theta_2^{(i)}\}_{i=B+1}^{N} \) which under mild regularity and if convergence occurs can be said to be drawn from the joint posterior distributions.
\end{algorithmic}
\end{algorithm}
\subsection{Using R and without relying on external software/packages (except the \texttt{coda} package), write the Gibbs sampler based on your pseudo-code in step }

\subsection{Please specify \texttt{set.seed(2025)} at the beginning of your code for reproducibility of your results. What is your acceptance rate?}

\subsection{Use the Geweke convergence diagnostic test on the generated chains and interpret your results.}

\subsection{Compute a point estimate for $\theta_1$ and a 95\% quantile-based credible interval}

\subsection{Using your generated chains, estimate the probability $P(\theta_1 > 0.5)$.}



\section{Part 2 - The Metropolis algorithm}
\subsection{Write the log posterior distribution \( \log p(\theta|D) \) and obtain analytically the gradient
\[\nabla_\theta \log p(\theta|D) = \left( \frac{\partial \log p(\theta|D)}{\partial \theta_1}, \frac{\partial \log p(\theta|D)}{\partial \theta_2} \right)^\top \]
and Hessian matrix:
\[\nabla^2_\theta \log p(\theta|D) = 
\begin{pmatrix}
\frac{\partial^2 \log p(\theta|D)}{\partial \theta_1^2} & \frac{\partial^2 \log p(\theta|D)}{\partial \theta_1 \partial \theta_2} \\
\frac{\partial^2 \log p(\theta|D)}{\partial \theta_2 \partial \theta_1} & \frac{\partial^2 \log p(\theta|D)}{\partial \theta_2^2} 
\end{pmatrix}.\]}
\textbf{Solution}\\
The log posterior distribution could be written as:
\[
\log(p(\theta|D)) \propto -8\theta_1^2\theta_2^2 - 0.5\theta_1^2 - 0.5\theta_2^2 + \cos(2\pi + 0.3)\theta_1\theta_2 + 0.3\theta_1 + 0.2\theta_2
\]
By taking the first and second-order partial derivatives with regard to \(\theta_1\) and \(\theta_2\), of \(\log p(\theta|D)\),
we obtain the gradient as follows:
\[
\nabla_\theta \log p(\theta|D) = \left( -16\theta_1\theta_2^2 - \theta_1 + \cos(2\pi + 0.3)\theta_2 + 0.3, -16\theta_1^2\theta_2 - \theta_2 + \cos(2\pi + 0.3)\theta_1 + 0.2 \right)^\top
\]
And the Hessian matrix is as follows:
\[
\nabla^2_\theta \log p(\theta|D) = 
\begin{pmatrix}
-16\theta_2^2 - 1 & -32\theta_1\theta_2 + \cos(2\pi + 0.3) \\
-32\theta_1\theta_2 + \cos(2\pi + 0.3) & -16\theta_1^2 - 1
\end{pmatrix}
\]

\subsection{Using the gradient and Hessian matrix obtained in the previous step, implement a Newton-Raphson algorithm in R to find the posterior mode of \( p(\theta|D) \) and denote by
\[
\theta^*_{NR} = (\theta^*_1, \theta^*_2)^\top
\]
the mode after convergence of the algorithm. (Note: round \( \theta^*_{NR} \) to five digits after the decimal point).}
\textbf{Solution}
The Pseudo-code of Newton-Raphson algorithm to find the mode of \( p(\theta|D) \) is as follows:

\begin{algorithm}
\begin{algorithmic}[1]
\State Set tolerance \( \epsilon \), arbitrary initial distance \( d \), initialize \( \theta^{(0)} \) and set \( m = 0 \).
\While{$d > \epsilon$}
    \State \( \theta^{(m+1)} = \theta^{(m)} - \left( \nabla^2 \log p(\theta^{(m)}|D) \right)^{-1} \nabla \log p(\theta^{(m)}|D) \)
    \State Compute distance \( d = \|\theta^{(m+1)} - \theta^{(m)}\| \).
\EndWhile
\State At convergence, return \( \theta^M \).
\end{algorithmic}
\end{algorithm}

\subsection{Compute a Laplace approximation to \( p(\theta|D) \) around \( \theta^*_{NR} \). Report the covariance matrix \( \Sigma^* \) of the Laplace approximation. (Note: round your results to five digits after the decimal point).}
\textbf{Solution}

\subsection{In R, write a random-walk Metropolis algorithm to explore the joint posterior \( p(\theta|D) \) using a Gaussian proposal with covariance matrix \( \tilde{\Sigma} = c\Sigma^* \). Use a chain of length \( M = 50,000 \) and tune \( c \) to (approximately) reach the optimal acceptance rate of 23\%. Please specify \textit{set.seed(1993)} for the sake of reproducibility of your results.}
\textbf{Solution}

\subsection{Using your generated chains, estimate the probability \( P\left(\frac{\theta_1}{\theta_2} > 0.45\right) \).}
\textbf{Solution}

\printbibliography

\end{document}
